{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5076e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from memory_profiler import memory_usage\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfe68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load sequence data from SPMF format file\"\"\"\n",
    "    sequences = []\n",
    "    current_sequence = []\n",
    "    current_event = set()\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                num = int(token)\n",
    "                \n",
    "                if num == -1:  # End of event\n",
    "                    if current_event:\n",
    "                        current_sequence.append(sorted(current_event))\n",
    "                        current_event = set()\n",
    "                elif num == -2:  # End of sequence\n",
    "                    if current_event:\n",
    "                        current_sequence.append(sorted(current_event))\n",
    "                        current_event = set()\n",
    "                    if current_sequence:\n",
    "                        sequences.append(current_sequence)\n",
    "                        current_sequence = []\n",
    "                else:  # Event ID\n",
    "                    current_event.add(num)\n",
    "    \n",
    "    # Handle any remaining data\n",
    "    if current_event:\n",
    "        current_sequence.append(sorted(current_event))\n",
    "    if current_sequence:\n",
    "        sequences.append(current_sequence)\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7489b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prefix_span(sequences, min_support, max_pattern_length):\n",
    "    \"\"\"\n",
    "    Main PrefixSpan function to mine sequential patterns\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences: list of sequences, each sequence is a list of itemsets\n",
    "    - min_support: minimum support threshold (absolute count)\n",
    "    - max_pattern_length: maximum length of patterns to mine\n",
    "    \n",
    "    Returns:\n",
    "    - List of (pattern, support) tuples\n",
    "    \"\"\"\n",
    "    # Results container\n",
    "    frequent_patterns = []\n",
    "    \n",
    "    # Get frequent items\n",
    "    all_items = set()\n",
    "    item_support = defaultdict(int)\n",
    "    \n",
    "    # First scan to find frequent 1-items\n",
    "    for sequence in sequences:\n",
    "        # For each sequence, an item can be counted only once\n",
    "        items_in_seq = set()\n",
    "        \n",
    "        for itemset in sequence:\n",
    "            for item in itemset:\n",
    "                items_in_seq.add(item)\n",
    "                all_items.add(item)\n",
    "        \n",
    "        for item in items_in_seq:\n",
    "            item_support[item] += 1\n",
    "    \n",
    "    # Filter to get only frequent items\n",
    "    frequent_items = {item: support for item, support in item_support.items() \n",
    "                     if support >= min_support}\n",
    "    \n",
    "    # Sort frequent items for consistent output\n",
    "    sorted_frequent_items = sorted(frequent_items.items())\n",
    "    \n",
    "    # Generate frequent 1-patterns\n",
    "    for item, support in sorted_frequent_items:\n",
    "        pattern = [item]\n",
    "        frequent_patterns.append((pattern, support))\n",
    "        \n",
    "        # Project database for this item\n",
    "        projected_db = []\n",
    "        for sequence in sequences:\n",
    "            # Find postfix starting from first occurrence of item\n",
    "            postfix = []\n",
    "            found = False\n",
    "            \n",
    "            for i, itemset in enumerate(sequence):\n",
    "                if item in itemset:\n",
    "                    found = True\n",
    "                    # Add remaining items from this itemset (if any)\n",
    "                    remaining = [x for x in itemset if x > item]\n",
    "                    if remaining:\n",
    "                        postfix.append(remaining)\n",
    "                    # Add remaining itemsets\n",
    "                    postfix.extend(sequence[i+1:])\n",
    "                    break\n",
    "            \n",
    "            if found and postfix:\n",
    "                projected_db.append(postfix)\n",
    "        \n",
    "        # Recursive pattern growth\n",
    "        if len(pattern) < max_pattern_length and projected_db:\n",
    "            prefix_span_rec(pattern, projected_db, min_support, max_pattern_length, frequent_patterns)\n",
    "    \n",
    "    return frequent_patterns\n",
    "\n",
    "def prefix_span_rec(pattern, projected_db, min_support, max_pattern_length, frequent_patterns):\n",
    "    \"\"\"Recursive pattern growth function for PrefixSpan\"\"\"\n",
    "    # Find all frequent items in the projected database\n",
    "    item_support = defaultdict(int)\n",
    "    \n",
    "    # Count support\n",
    "    for sequence in projected_db:\n",
    "        # Track items found in this sequence to avoid counting duplicates\n",
    "        found_items = set()\n",
    "        \n",
    "        for itemset in sequence:\n",
    "            for item in itemset:\n",
    "                if item not in found_items:\n",
    "                    item_support[item] += 1\n",
    "                    found_items.add(item)\n",
    "    \n",
    "    # Filter to get only frequent items\n",
    "    frequent_items = {item: support for item, support in item_support.items() \n",
    "                     if support >= min_support}\n",
    "    \n",
    "    # Sort frequent items for consistent output\n",
    "    sorted_frequent_items = sorted(frequent_items.items())\n",
    "    \n",
    "    # For each frequent item, extend pattern\n",
    "    for item, support in sorted_frequent_items:\n",
    "        new_pattern = pattern + [item]\n",
    "        frequent_patterns.append((new_pattern, support))\n",
    "        \n",
    "        # If maximum pattern length not reached, project database\n",
    "        if len(new_pattern) < max_pattern_length:\n",
    "            new_projected_db = []\n",
    "            for sequence in projected_db:\n",
    "                # Find postfix starting from first occurrence of item\n",
    "                postfix = []\n",
    "                found = False\n",
    "                \n",
    "                for i, itemset in enumerate(sequence):\n",
    "                    if item in itemset:\n",
    "                        found = True\n",
    "                        # Add remaining items from this itemset (if any)\n",
    "                        remaining = [x for x in itemset if x > item]\n",
    "                        if remaining:\n",
    "                            postfix.append(remaining)\n",
    "                        # Add remaining itemsets\n",
    "                        postfix.extend(sequence[i+1:])\n",
    "                        break\n",
    "                \n",
    "                if found and postfix:\n",
    "                    new_projected_db.append(postfix)\n",
    "            \n",
    "            # Recursive call if there are sequences in projected database\n",
    "            if new_projected_db:\n",
    "                prefix_span_rec(new_pattern, new_projected_db, min_support, max_pattern_length, frequent_patterns)\n",
    "\n",
    "def run_prefix_span(file_path, minsup, max_length=float('inf')):\n",
    "    \"\"\"Run PrefixSpan algorithm and track performance metrics\"\"\"\n",
    "    # Track performance\n",
    "    start_time = time.time()\n",
    "    start_mem = memory_usage()[0]\n",
    "    \n",
    "    # Load data\n",
    "    sequences = load_data(file_path)\n",
    "    \n",
    "    # Convert minsup to absolute count if it's a fraction\n",
    "    if 0 < minsup < 1:\n",
    "        minsup = ceil(minsup * len(sequences))\n",
    "    \n",
    "    # Use the corrected implementation\n",
    "    frequent_patterns = prefix_span(sequences, minsup, max_length)\n",
    "    \n",
    "    # Sort by support (descending)\n",
    "    frequent_patterns.sort(key=lambda x: (x[1], len(x[0])), reverse=True)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    end_time = time.time()\n",
    "    end_mem = memory_usage()[0]\n",
    "    memory_consumption = end_mem - start_mem\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    return frequent_patterns, len(sequences), memory_consumption, execution_time\n",
    "\n",
    "def format_pattern(pattern):\n",
    "    \"\"\"Format a pattern for display\"\"\"\n",
    "    return ' -> '.join(map(str, pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da5e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences from BmsWeb1.txt...\n",
      "\n",
      "=== Performance Summary ===\n",
      "Dataset: BmsWeb1.txt\n",
      "Minimum support: 0.1 (10.0%)\n",
      "Total sequences: 59601\n",
      "Total memory used: 21.62 MB\n",
      "Total execution time: 0.28 seconds\n",
      "Total frequent patterns found: 0\n",
      "\n",
      "Top 20 frequent patterns:\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "DATASET_DIR = \"/home/faiak/Desktop/Academic/Data-Mining/Assignment_5/Datasets/\"\n",
    "file_name = \"BmsWeb1.txt\"\n",
    "file_path = DATASET_DIR + file_name\n",
    "\n",
    "# Parameters\n",
    "min_sup = 0.1  # Minimum support threshold (50%)\n",
    "max_pattern_length = 100  # Maximum length to mine\n",
    "\n",
    "print(f\"Loading sequences from {file_name}...\")\n",
    "patterns, total_sequences, memory_usage_val, execution_time = run_prefix_span(file_path, min_sup, max_pattern_length)\n",
    "\n",
    "# Print performance summary\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Dataset: {file_name}\")\n",
    "print(f\"Minimum support: {min_sup} ({min_sup*100}%)\")\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "print(f\"Total memory used: {memory_usage_val:.2f} MB\")\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "print(f\"Total frequent patterns found: {len(patterns)}\")\n",
    "\n",
    "# Print top patterns\n",
    "print(\"\\nTop 20 frequent patterns:\")\n",
    "for i, (pattern, support) in enumerate(patterns[:20], 1):\n",
    "    print(f\"{i}. {format_pattern(pattern)}: {support} ({support/total_sequences:.2%})\")\n",
    "\n",
    "# Save results to output file\n",
    "O_file_path = \"/home/faiak/Desktop/Academic/Data-Mining/Assignment_5/output.log\"\n",
    "with open(O_file_path, \"a\") as f:\n",
    "    f.write(f\"PrefixSpan Results (Min_Sup = {min_sup})\\n\")\n",
    "    f.write(f\"Total memory used: {memory_usage_val:.2f} MB\\n\")\n",
    "    f.write(f\"Total execution time: {execution_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Total frequent patterns found: {len(patterns)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
