{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec064f5",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6fee16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, sqrt, pi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.label_probs = {}\n",
    "        self.cond_probs = {}\n",
    "        self.mean_store = {}\n",
    "        self.std_store = {}\n",
    "        self.is_continuous_map = {}\n",
    "        self.labels = []\n",
    "        self.attributes = []\n",
    "\n",
    "    def _is_continuous(self, X: pd.Series) -> bool:\n",
    "        return np.issubdtype(X.dtype, np.number) and (len(X.unique()) / len(X) > 0.01)\n",
    "\n",
    "    def _log_gaussian(self, x, mean, std):\n",
    "        if std < 1e-9:\n",
    "            return 0.0 if abs(x - mean) < 1e-9 else -np.inf\n",
    "        coeff = -0.5 * log(2 * pi) - log(std)\n",
    "        exponent = -((x - mean) ** 2) / (2 * std ** 2)\n",
    "        return coeff + exponent\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "        self.attributes = list(X_train.columns)\n",
    "        self.labels = y_train.iloc[:, 0].unique()\n",
    "        total_rows = len(y_train)\n",
    "\n",
    "        # Identify continuous/discrete attributes\n",
    "        for attr in self.attributes:\n",
    "            self.is_continuous_map[attr] = self._is_continuous(X_train[attr])\n",
    "\n",
    "        # Compute priors and conditional distributions\n",
    "        for label in self.labels:\n",
    "            label_mask = y_train.iloc[:, 0] == label\n",
    "            label_X = X_train[label_mask]\n",
    "            count = label_mask.sum()\n",
    "            self.label_probs[label] = log((count + self.alpha) / (total_rows + self.alpha * len(self.labels)))\n",
    "\n",
    "            for attr in self.attributes:\n",
    "                if self.is_continuous_map[attr]:\n",
    "                    mean = label_X[attr].mean()\n",
    "                    std = max(label_X[attr].std(ddof=0), 1e-3)\n",
    "                    self.mean_store[(attr, label)] = mean\n",
    "                    self.std_store[(attr, label)] = std\n",
    "                else:\n",
    "                    full_domain = X_train[attr].unique()\n",
    "                    value_counts = label_X[attr].value_counts()\n",
    "                    total = len(label_X)\n",
    "                    for val in full_domain:\n",
    "                        count = value_counts.get(val, 0)\n",
    "                        prob = (count + self.alpha) / (total + self.alpha * len(full_domain))\n",
    "                        self.cond_probs[(attr, val, label)] = log(prob)\n",
    "\n",
    "    def predict_single(self, row: pd.Series):\n",
    "        class_log_probs = {}\n",
    "        for label in self.labels:\n",
    "            log_prob = self.label_probs[label]\n",
    "            for attr in self.attributes:\n",
    "                val = row[attr]\n",
    "                if self.is_continuous_map[attr]:\n",
    "                    mean = self.mean_store.get((attr, label), 0.0)\n",
    "                    std = self.std_store.get((attr, label), 1e-3)\n",
    "                    log_prob += self._log_gaussian(val, mean, std)\n",
    "                else:\n",
    "                    log_prob += self.cond_probs.get((attr, val, label), log(self.alpha))\n",
    "            class_log_probs[label] = log_prob\n",
    "        return str(max(class_log_probs, key=class_log_probs.get))\n",
    "\n",
    "    def predict(self, X_test: pd.DataFrame):\n",
    "        return [self.predict_single(row) for _, row in X_test.iterrows()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c7b31",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _is_continous(X: pd.Series) -> bool:\n",
    "    return np.issubdtype(X.dtype, np.number) and (len(X.unique()) / len(X) > 0.01)\n",
    "\n",
    "        \n",
    "\n",
    "class DiscreteAttributeSelectionCriteria:\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = value \n",
    "        \n",
    "    def condition_satisfied(self, val):\n",
    "        if self.value == None:\n",
    "            raise ValueError(f\"value not set\")\n",
    "        \n",
    "        if self.value == val:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        \n",
    "class ContinuousAtrributeSelectionCriteria:\n",
    "    \n",
    "    def __init__(self, start_point, end_point):\n",
    "        self.start_point = start_point\n",
    "        self.end_point = end_point\n",
    "    \n",
    "    def condition_satisfied(self, val):\n",
    "        if self.start_point == None or self.end_point==None:\n",
    "            raise ValueError(f\"value not set\")\n",
    "        \n",
    "        if self.start_point <= val <= self.end_point:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20abb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "\n",
    "def info(D):\n",
    "    label_counts = D.iloc[:, -1].value_counts().to_dict()\n",
    "    info_val = 0\n",
    "    for label in label_counts:\n",
    "        pi = label_counts[label] / len(D)\n",
    "        info_val += - pi * log2(pi)\n",
    "    return info_val\n",
    "\n",
    "def info_A(D, attr):\n",
    "    info_A = 0\n",
    "    attr_values = D[attr].unique()\n",
    "    for attr_val in attr_values:\n",
    "        Dj = D[D[attr] == attr_val]\n",
    "        info_A += (len(Dj) / len(D)) * info(Dj)\n",
    "    return info_A\n",
    "\n",
    "def Gain(D, A):\n",
    "    return info(D) - info_A(D, A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from platform import node\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.isLeaf = False\n",
    "        self.split_attribute = None  \n",
    "        self.returning_class = None\n",
    "        self.attribute_selection_criteria = None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, multiple_splits_allowed=True):\n",
    "        self.root = None\n",
    "        self.multiple_splits_allowed = multiple_splits_allowed\n",
    "\n",
    "    def attribute_selection(self, D, attribute_list):\n",
    "        best_gain = -1\n",
    "        best_attr = None\n",
    "        best_criterion = None\n",
    "\n",
    "        for attr in attribute_list:\n",
    "            \n",
    "            if len(D[attr].unique()) <= 1:\n",
    "                continue\n",
    "\n",
    "            if _is_continous(D[attr]):\n",
    "                sorted_vals = np.sort(D[attr].dropna().unique())\n",
    "                split_points = [(sorted_vals[i] + sorted_vals[i+1]) / 2 for i in range(len(sorted_vals)-1)]\n",
    "\n",
    "                for split in split_points:\n",
    "                    D_left = D[D[attr] <= split]\n",
    "                    D_right = D[D[attr] > split]\n",
    "                    if len(D_left) == 0 or len(D_right) == 0:\n",
    "                        continue\n",
    "                    weighted_info = (len(D_left)/len(D)) * info(D_left) + (len(D_right)/len(D)) * info(D_right)\n",
    "                    gain = info(D) - weighted_info\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_attr = attr\n",
    "                        best_criterion = [split]  # Store best split point\n",
    "            else:\n",
    "                gain = Gain(D, attr)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_attr = attr\n",
    "                    best_criterion = D[attr].unique()\n",
    "\n",
    "        return best_criterion, best_attr\n",
    "\n",
    "    def build_tree(self, X_train, y_train):\n",
    "        # Combine features and labels into one DataFrame\n",
    "        D = pd.concat([X_train, y_train], axis=1)\n",
    "        attribute_list = set(X_train.columns)\n",
    "\n",
    "        def generate_decision_tree(D, attribute_list):\n",
    "            node = Node()\n",
    "\n",
    "            # Stopping condition 1: All samples have the same label\n",
    "            if len(D.iloc[:, -1].unique()) == 1:\n",
    "                node.isLeaf = True\n",
    "                node.returning_class = D.iloc[:, -1].iloc[0]\n",
    "                return node\n",
    "\n",
    "            # Stopping condition 2: No attributes left to split\n",
    "            if len(attribute_list) == 0:\n",
    "                node.isLeaf = True\n",
    "                majority_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                node.returning_class = majority_class\n",
    "                return node\n",
    "\n",
    "            best_criterion, best_attr = self.attribute_selection(D, attribute_list)\n",
    "            \n",
    "            node.split_attribute = best_attr  # Store the attribute name\n",
    "            \n",
    "            if not self.multiple_splits_allowed:\n",
    "                attribute_list = attribute_list - {best_attr}\n",
    "\n",
    "            # If no attribute gives positive gain, make leaf node with majority class\n",
    "            if best_attr is None or len(best_criterion) == 0:\n",
    "                node.isLeaf = True\n",
    "                node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                return node\n",
    "\n",
    "            \n",
    "            if _is_continous(D[best_attr]):\n",
    "                split = best_criterion[0]\n",
    "                \n",
    "                node.attribute_selection_criteria = ContinuousAtrributeSelectionCriteria(start_point=-float('inf'), end_point=split)\n",
    "                \n",
    "                D_left = D[D[best_attr] <= split]\n",
    "                D_right = D[D[best_attr] > split]\n",
    "                \n",
    "                if len(D_left) == 0 or len(D_right) == 0:\n",
    "                    node.isLeaf = True\n",
    "                    node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                    return node\n",
    "\n",
    "\n",
    "                node.children['left'] = generate_decision_tree(D_left, attribute_list.copy())\n",
    "                node.children['right'] = generate_decision_tree(D_right, attribute_list.copy())\n",
    "            else:\n",
    "                node.attribute_selection_criteria = DiscreteAttributeSelectionCriteria(value=best_criterion[0])\n",
    "            # If multiple splits are NOT allowed, remove the chosen attribute\n",
    "                \n",
    "\n",
    "                # Split dataset by each attribute value and recurse\n",
    "                for attr_val in best_criterion:\n",
    "                    D_j = D[D[best_attr] == attr_val]\n",
    "\n",
    "                    # If no samples in this subset, create leaf with majority class of parent\n",
    "                    if len(D_j) == 0:\n",
    "                        leaf_node = Node()\n",
    "                        leaf_node.isLeaf = True\n",
    "                        leaf_node.returning_class = D.iloc[:, -1].value_counts().idxmax()\n",
    "                        node.children[attr_val] = leaf_node\n",
    "                    else:\n",
    "                        node.children[attr_val] = generate_decision_tree(D_j, attribute_list.copy())\n",
    "\n",
    "            return node\n",
    "\n",
    "        self.root = generate_decision_tree(D, attribute_list)\n",
    "\n",
    "    def _majority_class(self, node):\n",
    "        from collections import Counter\n",
    "\n",
    "        def collect_leaf_classes(n):\n",
    "            if n.isLeaf:\n",
    "                return [n.returning_class]\n",
    "            labels = []\n",
    "            for child in n.children.values():\n",
    "                labels.extend(collect_leaf_classes(child))\n",
    "            return labels\n",
    "\n",
    "        leaf_classes = collect_leaf_classes(node)\n",
    "        if not leaf_classes:\n",
    "            return None\n",
    "        return Counter(leaf_classes).most_common(1)[0][0]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        node = self.root\n",
    "        while not node.isLeaf:\n",
    "            attr = node.split_attribute  \n",
    "            val = x[attr]                \n",
    "\n",
    "            if isinstance(node.attribute_selection_criteria, ContinuousAtrributeSelectionCriteria):\n",
    "                if node.attribute_selection_criteria.condition_satisfied(val):\n",
    "                    node = node.children['left']\n",
    "                else:\n",
    "                    node = node.children['right']\n",
    "            elif isinstance(node.attribute_selection_criteria, DiscreteAttributeSelectionCriteria):\n",
    "                if node.attribute_selection_criteria.condition_satisfied(val):\n",
    "                    node = node.children[val]\n",
    "                else:\n",
    "                    return self._majority_class(node)\n",
    "\n",
    "        return node.returning_class\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self.predict_single(row) for _, row in X_test.iterrows()]\n",
    "        return pd.Series(predictions, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feac44c",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "\n",
    "def __train_test_split(X, y, test_size = 0.2, shuffle_and_stratify = True):\n",
    "    \n",
    "    if test_size < 0 or test_size > 1:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "   \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(\"Features and targets must have the same length.\")\n",
    "\n",
    "    \n",
    "    if shuffle_and_stratify == False:\n",
    "    \n",
    "        train_size = 1 - test_size\n",
    "        train_index = int(len(X) * train_size)\n",
    "        \n",
    "        X_train = X[0: train_index]\n",
    "        X_test = X[train_index:]\n",
    "        \n",
    "        y_train = y[0: train_index]\n",
    "        y_test = y[train_index:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        labels = y.iloc[:,0].unique()\n",
    "        X_train = pd.DataFrame(columns=X.columns)\n",
    "        y_train = pd.DataFrame(columns=y.columns)\n",
    "        X_test = pd.DataFrame(columns=X.columns)\n",
    "        y_test = pd.DataFrame(columns=y.columns)\n",
    "        \n",
    "        train_size = 1 - test_size\n",
    "        \n",
    "\n",
    "        for label in labels :\n",
    "            y_rows = y[y.iloc[:,0] == label]            \n",
    "            X_rows = X.loc[y_rows.index]\n",
    "            \n",
    "            train_index = int(len(X_rows) * train_size)\n",
    "            \n",
    "            X_train = pd.concat([X_train, X_rows.iloc[:train_index]], ignore_index=False)\n",
    "            y_train = pd.concat([y_train, y_rows.iloc[:train_index]] , ignore_index=False)\n",
    "            \n",
    "            X_test = pd.concat([X_test, X_rows[train_index:]], ignore_index=False)\n",
    "            y_test = pd.concat([y_test, y_rows[train_index:]], ignore_index=False)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9758f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 101, 'name': 'Tic-Tac-Toe Endgame', 'repository_url': 'https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame', 'data_url': 'https://archive.ics.uci.edu/static/public/101/data.csv', 'abstract': 'Binary classification task on possible configurations of tic-tac-toe game', 'area': 'Games', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 958, 'num_features': 9, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1991, 'last_updated': 'Mon Aug 19 1991', 'dataset_doi': '10.24432/C5688J', 'creators': ['David Aha'], 'intro_paper': None, 'additional_info': {'summary': 'This database encodes the complete set of possible board configurations at the end of tic-tac-toe games, where \"x\" is assumed to have played first.  The target concept is \"win for x\" (i.e., true when \"x\" has one of 8 possible ways to create a \"three-in-a-row\").  \\r\\n\\r\\nInterestingly, this raw database gives a stripped-down decision tree algorithm (e.g., ID3) fits.  However, the rule-based CN2 algorithm, the simple IB1 instance-based learning algorithm, and the CITRE feature-constructing decision tree algorithm perform well on it.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '    1. top-left-square: {x,o,b}\\r\\n    2. top-middle-square: {x,o,b}\\r\\n    3. top-right-square: {x,o,b}\\r\\n    4. middle-left-square: {x,o,b}\\r\\n    5. middle-middle-square: {x,o,b}\\r\\n    6. middle-right-square: {x,o,b}\\r\\n    7. bottom-left-square: {x,o,b}\\r\\n    8. bottom-middle-square: {x,o,b}\\r\\n    9. bottom-right-square: {x,o,b}\\r\\n   10. Class: {positive,negative}', 'citation': None}}\n",
      "                   name     role         type demographic description units  \\\n",
      "0                 class   Target  Categorical        None        None  None   \n",
      "1       top-left-square  Feature  Categorical        None        None  None   \n",
      "2     top-middle-square  Feature  Categorical        None        None  None   \n",
      "3      top-right-square  Feature  Categorical        None        None  None   \n",
      "4    middle-left-square  Feature  Categorical        None        None  None   \n",
      "5  middle-middle-square  Feature  Categorical        None        None  None   \n",
      "6   middle-right-square  Feature  Categorical        None        None  None   \n",
      "7    bottom-left-square  Feature  Categorical        None        None  None   \n",
      "8  bottom-middle-square  Feature  Categorical        None        None  None   \n",
      "9   bottom-right-square  Feature  Categorical        None        None  None   \n",
      "\n",
      "  missing_values  \n",
      "0             no  \n",
      "1             no  \n",
      "2             no  \n",
      "3             no  \n",
      "4             no  \n",
      "5             no  \n",
      "6             no  \n",
      "7             no  \n",
      "8             no  \n",
      "9             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "ucirepo_ids = {\n",
    "    \"iris\": 53,\n",
    "    \"heart_disease\": 45,\n",
    "    \"molecular_biology\": 69,\n",
    "    \"breast_cancer\": 17,\n",
    "    \"adult\": 2,\n",
    "    \"bank_marketing\": 222,\n",
    "    \"wine\": 109,\n",
    "    \"mushroom\": 73,\n",
    "    \"solar_flare\": 89,\n",
    "    \"tic-tac-toe\":101\n",
    "}\n",
    "\n",
    "\n",
    "DATASET_NAME = \"tic-tac-toe\"  # Example dataset name\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "\n",
    "def fetch_dataframe(dataframe_name):\n",
    "    \n",
    "    if dataframe_name in ucirepo_ids:\n",
    "        # fetch dataset \n",
    "        df = fetch_ucirepo(id=ucirepo_ids[dataframe_name],) \n",
    "\n",
    "        # # data (as pandas dataframes) \n",
    "        X = df.data.features \n",
    "        y = df.data.targets \n",
    "        \n",
    "        # metadata \n",
    "        print(df.metadata) \n",
    "        \n",
    "        # variable information \n",
    "        print(df.variables) \n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataframe_name}' not found in UCI repository.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = fetch_dataframe(DATASET_NAME)\n",
    "\n",
    "X = df.data.features\n",
    "y = df.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = __train_test_split(X, y , test_size=TEST_SIZE, shuffle_and_stratify=True)\n",
    "\n",
    "naive_bayes_classifier = NaiveBayesClassifier(alpha=1)\n",
    "decision_tree_classifier = DecisionTreeClassifier(multiple_splits_allowed=True)\n",
    "\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "decision_tree_classifier.build_tree(X_train, y_train)\n",
    "\n",
    "y_pred_nb = naive_bayes_classifier.predict(X_test)\n",
    "y_pred_dt = decision_tree_classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ba050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------+---------------------+--------------------+---------+\n",
      "|  Label   |       NB Acc       |      NB Prec       |       NB Rec       |       NB F1        |      NB Spec       |       NB AUC       |       DT Acc       |       DT Prec       |       DT Rec       |        DT F1        |       DT Spec       |       DT AUC       | Support |\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------+---------------------+--------------------+---------+\n",
      "| negative | 0.7046632124352331 | 0.5892857142857143 | 0.4925373134328358 | 0.5365853658536586 | 0.8174603174603174 | 0.6549988154465767 | 0.7196152333399539 | 0.35094330783208605 | 0.1923244781285237 | 0.24847778674069068 |  0.8409435358717914 | 0.5166340070001576 |    67   |\n",
      "| positive | 0.7046632124352331 | 0.7518248175182481 | 0.8174603174603174 | 0.7832699619771863 | 0.4925373134328358 | 0.6549988154465767 | 0.7128577507777049 |  0.6528497409326425 |        1.0         |  0.7899686520376175 | 0.14738251541680011 |  0.5736912577084   |   126   |\n",
      "|  ------  |       ------       |       ------       |       ------       |       ------       |       ------       |       ------       |       ------       |        ------       |       ------       |        ------       |        ------       |       ------       |  ------ |\n",
      "| OVERALL  |       0.705        |                    |                    |                    |                    |                    |       0.653        |                     |                    |                     |                     |                    |   193   |\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------+---------------------+--------------------+---------+\n",
      "✅ Comparison report saved to mushroom_comparison_report.txt\n",
      "Naive Bayes Accuracy: 0.705\n",
      "Decision Tree Accuracy: 0.653\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def manual_metrics(y_true, y_pred, label):\n",
    "    P = N = TP = FP = TN = FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        true_label = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "        pred_label = y_pred.iloc[i] if hasattr(y_pred, 'iloc') else y_pred[i]\n",
    "\n",
    "        if true_label == label:\n",
    "            P += 1\n",
    "        else:\n",
    "            N += 1\n",
    "\n",
    "        if true_label == label and pred_label == label:\n",
    "            TP += 1\n",
    "        elif true_label == label and pred_label != label:\n",
    "            FN += 1\n",
    "        elif true_label != label and pred_label != label:\n",
    "            TN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "            \n",
    "\n",
    "\n",
    "    accuracy = (TP + TN) / (P + N) if (P + N) > 0 else 0.0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / P if P > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    f1_score  = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    auc       = (recall + specificity) / 2\n",
    "    support   = P\n",
    "\n",
    "    return accuracy, precision, recall, f1_score, specificity, auc, support\n",
    "\n",
    "\n",
    "\n",
    "def compare_classifiers_manual(y_true, y_pred_nb, y_pred_dt, dataset_name=\"dataset\"):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.iloc[:, 0]\n",
    "\n",
    "    if isinstance(y_pred_nb, list):\n",
    "        y_pred_nb = pd.Series(y_pred_nb)\n",
    "    if isinstance(y_pred_dt, list):\n",
    "        y_pred_dt = pd.Series(y_pred_dt)\n",
    "\n",
    "    labels = sorted(pd.Series(y_true).unique())\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\n",
    "        \"Label\",\n",
    "        \"NB Acc\", \"NB Prec\", \"NB Rec\", \"NB F1\", \"NB Spec\", \"NB AUC\",\n",
    "        \"DT Acc\", \"DT Prec\", \"DT Rec\", \"DT F1\", \"DT Spec\", \"DT AUC\",\n",
    "        \"Support\"\n",
    "    ]\n",
    "\n",
    "    for label in labels:\n",
    "        nb_acc, nb_prec, nb_rec, nb_f1, nb_spec, nb_auc, support = manual_metrics(y_true, y_pred_nb, label)\n",
    "        dt_acc, dt_prec, dt_rec, dt_f1, dt_spec, dt_auc, _       = manual_metrics(y_true, y_pred_dt, label)\n",
    "        \n",
    "        \n",
    "        table.add_row([\n",
    "            label,\n",
    "            nb_acc, nb_prec, nb_rec, nb_f1, nb_spec, nb_auc,\n",
    "            dt_acc, dt_prec, dt_rec, dt_f1, dt_spec, dt_auc,\n",
    "            support\n",
    "        ])\n",
    "\n",
    "\n",
    "    # Add overall accuracy\n",
    "    nb_overall_acc = accuracy_score(y_true, y_pred_nb)\n",
    "    dt_overall_acc = accuracy_score(y_true, y_pred_dt)\n",
    "    table.add_row([\"-\" * 6] * len(table.field_names))  # separator row\n",
    "    table.add_row([\n",
    "        \"OVERALL\",\n",
    "        round(nb_overall_acc, 3), \"\", \"\", \"\", \"\", \"\",\n",
    "        round(dt_overall_acc, 3), \"\", \"\", \"\", \"\", \"\",\n",
    "        len(y_true)\n",
    "    ])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "    # Save to .txt file\n",
    "    save_path = f\"{dataset_name}_comparison_report.txt\"\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(str(table))\n",
    "    print(f\"✅ Comparison report saved to {save_path}\")\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "report = compare_classifiers_manual(y_test, y_pred_nb, y_pred_dt, dataset_name=DATASET_NAME)\n",
    "# print(report)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.3f}\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
